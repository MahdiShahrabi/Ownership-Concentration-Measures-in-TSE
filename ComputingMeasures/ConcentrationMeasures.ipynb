{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import jdatetime as jd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import pickle\n",
    "import io\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import power_index_calculator as px\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Arbic Characters to Persian Characters !\n",
    "## Credit to \"https://github.com/rezakamalifard/Persian/blob/master/persian/persian.py\"\n",
    "import re\n",
    "def convert_ar_characters(input_str):\n",
    "    mapping = {\n",
    "        'ك': 'ک',\n",
    "        'دِ': 'د',\n",
    "        'بِ': 'ب',\n",
    "        'زِ': 'ز',\n",
    "        'ذِ': 'ذ',\n",
    "        'شِ': 'ش',\n",
    "        'سِ': 'س',\n",
    "        'ى': 'ی',\n",
    "        'ي': 'ی'\n",
    "    }\n",
    "    return _multiple_replace(mapping, input_str)\n",
    "\n",
    "def _multiple_replace(mapping, text):\n",
    "    pattern = \"|\".join(map(re.escape, mapping.keys()))\n",
    "    return re.sub(pattern, lambda m: mapping[m.group()], str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Balancesheet Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to read different file and prepare them\n",
    "def read_blnc_data(file='98.txt',path=r\"C:\\Users\\Mahdi\\OneDrive\\Master Thesis\\Data\"):\n",
    "\n",
    "    os.chdir(path)\n",
    "    with open(file,encoding=\"utf8\") as f:\n",
    "        fileobject = io.StringIO(f.read())\n",
    "\n",
    "    BlncData = pd.read_csv(fileobject, sep='\\t',  lineterminator='\\n', names=None)\n",
    "    \n",
    "    # Selecting Columns\n",
    "    BlncData = BlncData[['نماد', 'سال مالی', 'تاریخ مصوب','جمع دارایی‌های جاری',\n",
    "           'سرمایه گذاری‌ها و سایر دارایی‌ها', 'خالص دارایی‌های ثابت',\n",
    "           'جمع دارایی‌های غیر جاری', 'جمع کل دارایی‌ها', 'جمع بدهی‌های جاری',\n",
    "           'جمع بدهی‌های غیر جاری', 'جمع کل بدهی‌ها', 'سرمایه',\n",
    "           'سود و زیان انباشته', 'اندوخته قانونی',\n",
    "           'جمع حقوق صاحبان سهام در پایان سال مالی',\n",
    "           'جمع کل بدهی‌ها و حقوق صاحبان سهام',\n",
    "           'جمع حقوق صاحبان سهام مصوب (در مجمع عادی)']]\n",
    "    \n",
    "    # renaming columns\n",
    "    BlncData.rename(columns={'نماد':'Symbol','سال مالی':'Fin_year','جمع دارایی‌های جاری':'Tot_current_asset','تاریخ مصوب':'approve_date',\n",
    "                             'خالص دارایی‌های ثابت':'Net_fixed_assed','سرمایه گذاری‌ها و سایر دارایی‌ها':'other_asset',\n",
    "                             'جمع بدهی‌های جاری':'Tot_current_lib','جمع کل دارایی‌ها':'Tot_asset','جمع دارایی‌های غیر جاری':'Tot_uncurrent_asset',\n",
    "                             'سرمایه':'Capital','حقوق عمومی':'Public_rights','جمع کل بدهی‌ها':'Tot_lib','جمع بدهی‌های غیر جاری':'Tot_uncurrent_lib',\n",
    "                             'سایر اندوخته‌ها':'Other_saving','اندوخته قانونی':'Reserved_saving','سود و زیان انباشته':'Comulated_profit_loss',\n",
    "                             'جمع حقوق صاحبان سهام در پایان سال مالی':'Equity_at_year_end','جمع کل بدهی‌ها و حقوق صاحبان سهام':'Debt_Equity',\n",
    "                              'جمع حقوق صاحبان سهام مصوب (در مجمع عادی)':'Debt_Equity_normal'},inplace=True)\n",
    "\n",
    "    # DataOrg.Symbol: convert_ar_characters(x)\n",
    "    Names = BlncData.Symbol.drop_duplicates()\n",
    "    Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "    BlncData_Symbol_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "    BlncData['Symbol'] = BlncData.Symbol.map(BlncData_Symbol_ArtoFa_dict)\n",
    "\n",
    "    # Dates\n",
    "    BlncData = BlncData[~pd.isnull(BlncData.Fin_year)]\n",
    "    BlncData.Fin_year = BlncData.Fin_year.apply(lambda x: jd.date(day=int(x[8:10]), month=int(x[5:7]),year=int(x[0:4])))\n",
    "\n",
    "    BlncData = BlncData[~pd.isnull(BlncData.approve_date)]\n",
    "    BlncData.approve_date = BlncData.approve_date.apply(lambda x: jd.date(day=int(x[8:10]), month=int(x[5:7]),year=int(x[0:4])))\n",
    "    \n",
    "    # changing to int\n",
    "    for x in BlncData.columns[3:]:\n",
    "        BlncData = BlncData[~pd.isnull(BlncData[x])]\n",
    "        BlncData[x] = BlncData[x].apply(lambda x: int(x.replace(',','')))\n",
    "        \n",
    "    return(BlncData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "blnc_data = read_blnc_data(file='98.txt')\n",
    "blnc_data['book_value'] = blnc_data.Tot_asset-blnc_data.Tot_lib\n",
    "blnc_data = blnc_data[blnc_data.book_value>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Shareholder Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading DATA\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\OneDrive\\Master Thesis\\Data\")\n",
    "SDATA = pd.read_csv(\"Shareholder97.csv\",index_col=0)\n",
    "\n",
    "# Conver date from string to jdatetime\n",
    "SDATA['True_Date'] = pd.to_datetime(SDATA['True_Date'], format='%Y-%m-%d')\n",
    "G = SDATA.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "DataOrg_date_GtoJ_dict = dict(zip(G,J))\n",
    "SDATA['Jalali_Date']=SDATA.True_Date.map(DataOrg_date_GtoJ_dict)\n",
    "\n",
    "SDATA.drop(columns=['High', 'Low', 'Open', 'Last', 'Volume', 'close',\n",
    "       'True_Date', 'year', 'month', 'day', 'Fill_Flag','Unadjusted_close','chnk_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering and keeping symbols that we have data in both datasets\n",
    "sym_list = list(set(blnc_data.Symbol).intersection(set(SDATA.Symbol.drop_duplicates())))\n",
    "SDATA = SDATA[SDATA.Symbol.isin(sym_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframe for saving concentration mearsurs\n",
    "CMdf = SDATA.groupby('Symbol',as_index=False).agg({'Id_tse':'first','percent':'sum','ShareHolder':'count'}).rename(columns={'ShareHolder':'Num_holders','percent':'sum_over1'})\n",
    "CMdf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Concentration Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Largest Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':'max'}).rename(columns={'percent':'Largest_Owner'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- First/Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_max(data,nth=1,interval=False):\n",
    "    data = data.sort_values(ascending=False)\n",
    "    if interval:\n",
    "        return(np.round(data.iloc[min(nth[0]-1,len(data)-1):min(nth[1],len(data))],2))\n",
    "    else:\n",
    "        return(np.round(data.iloc[min(nth-1,len(data)-1)],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:618: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3936: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':{lambda x: max(x)/nth_max(x,nth=2,interval=False)}}).rename(columns={'percent':'First_Second'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left').rename(columns={('First_Second', '<lambda>'):'First_Second'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- First/Sumtwofour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':{lambda x: max(x)/sum(nth_max(x,nth=[2,4],interval=True))}}).rename(columns={'percent':'First_Sumtwofour'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left').rename(columns={('First_Sumtwofour', '<lambda>'):'First_Sumtwofour'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Sumfive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':{lambda x: sum(nth_max(x,nth=[1,5],interval=True))/100}}).rename(columns={'percent':'Sumfive'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left').rename(columns={('Sumfive', '<lambda>'):'Sumfive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate gini coeficient using Deaton 1997:\n",
    "$$ \\gamma = \\frac{N+1}{N-1} - \\frac{2}{\\mu\\times N\\times(N-1)}\\sum_{i=1}^N{\\rho_ix_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(data):\n",
    "    data.sort(reverse = True)\n",
    "    N = len(data)\n",
    "    mu = np.mean(data)\n",
    "    ser = [(i+1)*data[i] for i in range(len(data))]\n",
    "    try:\n",
    "        gamma = (N+1)/(N-1)-(2*sum(ser))/(mu*N*(N-1))\n",
    "    except:\n",
    "        gamma = 0\n",
    "    return(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':{lambda x: gini(list(x))}}).rename(columns={'percent':'Gini'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left').rename(columns={('Gini', '<lambda>'):'Gini'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Herfindhal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SDATA.groupby('Symbol',as_index=False).agg({'percent':{lambda x: sum([(t/100)**2 for t in list(x)])}}).rename(columns={'percent':'Herfindhal'})\n",
    "CMdf = pd.merge(CMdf,temp,left_on='Symbol',right_on='Symbol',how='left').rename(columns={('Herfindhal', '<lambda>'):'Herfindhal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley-Shubik and Banzhaf Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Dict\n",
    "URL_shapley={'direct':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ssdirect.cgi\",\n",
    "             'genf':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ssgenf.cgi\",\n",
    "             'mmle':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ssmmle.cgi\",\n",
    "             'ocean':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ssocean.cgi\",\n",
    "             'concentrated':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ssmmle.cgi\"}\n",
    "\n",
    "\n",
    "def find_shapley(percent,how='direct',quota = 50.01,major_mode='number',major_thr=20,concentration_point=0.99,time_pnt=False,fast_mode = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function for finding Shapley-Shbik Index.\n",
    "    \n",
    "    This functions uses David Leech website to calculate Shapley-Shubik index.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    percent: list, voting rights\n",
    "    \n",
    "    how: str, 'direct', 'concentrated', 'ocean', 'genf', and 'mmle'\n",
    "    \n",
    "    quota: float\n",
    "    \n",
    "    major_mode: 'percent' or 'number'\n",
    "    \n",
    "    major_thr: if major_mode is 'percnet'--> float\n",
    "               if major_mode is 'number'--> int\n",
    "            \n",
    "    concentration_point: float, [less than 1]\n",
    "    \"\"\"\n",
    "    df = percent\n",
    "    \n",
    "    # sorting\n",
    "    percent.sort(reverse=True)\n",
    "    \n",
    "    # Fast mode calculates\n",
    "    if percent[0]>=quota and fast_mode:\n",
    "        if how is not 'concentrated' and how is not 'ocean':\n",
    "            out = pd.DataFrame(data={'Weight':percent,\n",
    "                             'Shapley-Shubik Index':[1]+[0]*(len(percent)-1)})\n",
    "        elif how is 'concentrated':\n",
    "            unassigned = 100 - sum(percent)\n",
    "            cons_point = concentration_point\n",
    "            number = int(np.floor(unassigned/cons_point))\n",
    "            residual = np.round(unassigned - number*cons_point,2)\n",
    "            percent = percent+[cons_point]*number+[residual]\n",
    "            out = pd.DataFrame(data={'Weight':percent,\n",
    "                             'Shapley-Shubik Index':[1]+[0]*(len(percent)-1)})\n",
    "        elif how is 'ocean':\n",
    "            out = pd.DataFrame(data={'Weight':percent+['Ocean'],\n",
    "                             'Shapley-Shubik Index':[1]+[0]*(len(percent))})\n",
    "        if time_pnt:\n",
    "            print(' Fast_mode on!')\n",
    "        return(out)\n",
    "    \n",
    "    # Checking size of input\n",
    "    if len(df)<=1 and how is not 'ocean' and how is not 'concentrated':\n",
    "        return('Error: Length Error!')\n",
    "    \n",
    "    \n",
    "    ## Preparing website inputs\n",
    "    if how is 'direct':\n",
    "        prc = percent\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        payload = {'numberofplayers': len(df),\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}\n",
    "        \n",
    "        \n",
    "    elif how is 'genf':\n",
    "        prc = [int(x*100)for x in percent]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        payload = {'numberofplayers': len(df),\n",
    "                   'quota': int(quota*100),\n",
    "                   'textarea': prc_str}    \n",
    "        \n",
    "        \n",
    "    elif how is 'mmle':\n",
    "        prc = [x for x in percent]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        if major_mode=='percent':\n",
    "            Majors = len([x for x in prc if x>=major_thr])\n",
    "            Minors = len(df) - Majors\n",
    "        elif major_mode=='number':\n",
    "            Majors = len(prc[0:min(major_thr,len(prc))])\n",
    "            Minors = len(prc) - Majors\n",
    "        payload = {'numberofplayers': Majors,#majo3\n",
    "                   'numberofplayers2': Minors,#minor\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}        \n",
    "        \n",
    "        \n",
    "    elif how is 'concentrated':\n",
    "        prc = [x for x in percent]\n",
    "        unassigned = 100 - sum(prc)\n",
    "        cons_point = concentration_point\n",
    "        number = int(np.floor(unassigned/cons_point))\n",
    "        residual = np.round(unassigned - number*cons_point,2)\n",
    "        prc = prc+[cons_point]*number+[residual]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        if major_mode=='percent':\n",
    "            Majors = len([x for x in prc if x>=major_thr])\n",
    "            Minors = len(prc) - Majors\n",
    "        elif major_mode=='number':\n",
    "            Majors = len(prc[0:min(major_thr,len(prc))])\n",
    "            Minors = len(prc) - Majors\n",
    "        payload = {'numberofplayers': Majors,#majo3\n",
    "                   'numberofplayers2': Minors,#minor\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}    \n",
    "    \n",
    "    \n",
    "    elif how is 'ocean':\n",
    "        prc = [x for x in percent]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        total_weight = 100\n",
    "        payload = {'numberofplayers': len(df),# number of atomic players\n",
    "                   'totalweight': total_weight,\n",
    "                   'quota': 50.1,\n",
    "                   'textarea': prc_str}\n",
    "    \n",
    "    \n",
    "    # website url\n",
    "    url = URL_shapley[how]\n",
    "    \n",
    "    # Making request\n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, data = payload)\n",
    "    except:\n",
    "        return('Error: request error!')\n",
    "    if time_pnt:\n",
    "        print(' It took about ',np.round(response.elapsed.microseconds/1e6,2), 'seconds')\n",
    "        \n",
    "    # Parshing output html of wevsite\n",
    "    parsed_html = BeautifulSoup(response.text.encode('utf8'))\n",
    "    \n",
    "     # Finding rows or error message\n",
    "    if parsed_html('tr'):\n",
    "        rows = parsed_html('tr')\n",
    "    else:\n",
    "        return('Error: '+parsed_html.find('p').text)\n",
    "    \n",
    "    # Extracting rows to a list of lists\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        if row.th:\n",
    "            cols = row.find_all('th')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            if len(cols) is 1:\n",
    "                data.append([cols[0],''])\n",
    "            else:\n",
    "                data.append([ele for ele in cols if ele]) # Get rid of empty valuespty values\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "    \n",
    "    # Converting list of lists to a dataframe\n",
    "    try:\n",
    "        if how is 'mmle' or how is 'concentrated':\n",
    "            out = pd.DataFrame(data[1:(len(data)-1)], columns=data[0])\n",
    "            out.iloc[:,0] = prc\n",
    "            \n",
    "        elif how is 'ocean':\n",
    "            del data[0]\n",
    "            del data[-2]\n",
    "            data[-1][0] = 'Ocean'\n",
    "            out = pd.DataFrame(data[1:len(data)], columns=data[0])\n",
    "            \n",
    "        else: \n",
    "            out = pd.DataFrame(data[1:], columns=data[0])\n",
    "            out.iloc[:,0] = prc\n",
    "        \n",
    "        return(out)\n",
    "    \n",
    "    except:\n",
    "        return('Error: creating dataFrame error! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL Dict\n",
    "URL_banzhaf={'direct':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ipdirect.cgi\",\n",
    "             'genf':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ipgenf.cgi\",\n",
    "             'mmle':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ipmmle.cgi\",\n",
    "             'concentrated':\"https://mywebpages.csv.warwick.ac.uk/cgi-vpi/ipmmle.cgi\"}\n",
    "\n",
    "\n",
    "def find_banzhaf(percent,how='direct',quota = 50.01,major_mode='number',major_thr=20,concentration_point=0.99,time_pnt=False,fast_mode = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function for finding banzhaf Index.\n",
    "    \n",
    "    This functions uses David Leech website to calculate banzhaf index.\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    percent: list, voting rights\n",
    "    \n",
    "    how: str, 'direct', 'concentrated', 'ocean', 'genf', and 'mmle'\n",
    "    \n",
    "    quota: float\n",
    "    \n",
    "    major_mode: 'percent' or 'number'\n",
    "    \n",
    "    major_thr: if major_mode is 'percnet'--> float\n",
    "               if major_mode is 'number'--> int\n",
    "            \n",
    "    concentration_point: float, [less than 1]\n",
    "    \"\"\"\n",
    "    df = percent\n",
    "    \n",
    "    # sorting\n",
    "    percent.sort(reverse=True)\n",
    "    \n",
    "    # Fast mode calculates\n",
    "    if percent[0]>=quota and fast_mode:\n",
    "        if how is not 'concentrated':\n",
    "            out = pd.DataFrame(data={'Weight':percent,\n",
    "                                     'Abs_Banzhaf':[1]+[0]*(len(percent)-1),\n",
    "                                     'Norm_Banzhaf':[1]+[0]*(len(percent)-1),\n",
    "                                     'Coleman_Prevent':[1]+[0]*(len(percent)-1),\n",
    "                                     'Coleman_Initiate':[1]+[0]*(len(percent)-1)})\n",
    "        elif how is 'concentrated':\n",
    "            unassigned = 100 - sum(percent)\n",
    "            cons_point = concentration_point\n",
    "            number = int(np.floor(unassigned/cons_point))\n",
    "            residual = np.round(unassigned - number*cons_point,2)\n",
    "            percent = percent+[cons_point]*number+[residual]\n",
    "            out = pd.DataFrame(data={'Weight':percent,\n",
    "                                     'Abs_Banzhaf':[1]+[0]*(len(percent)-1),\n",
    "                                     'Norm_Banzhaf':[1]+[0]*(len(percent)-1),\n",
    "                                     'Coleman_Prevent':[1]+[0]*(len(percent)-1),\n",
    "                                     'Coleman_Initiate':[1]+[0]*(len(percent)-1)})\n",
    "        if time_pnt:\n",
    "            print('Fast_mode on!')\n",
    "        return(out)\n",
    "    \n",
    "    # Checking size of input\n",
    "    if len(df)<=1 and how is not 'concentrated':\n",
    "        return('Error: Length Error!')\n",
    "    \n",
    "    \n",
    "    ## Preparing website inputs\n",
    "    if how is 'direct':\n",
    "        prc = percent\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        payload = {'numberofplayers': len(df),\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}\n",
    "        \n",
    "        \n",
    "    elif how is 'genf':\n",
    "        prc = [int(x*100)for x in percent]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        payload = {'numberofplayers': len(df),\n",
    "                   'quota': int(quota*100),\n",
    "                   'textarea': prc_str}    \n",
    "        \n",
    "        \n",
    "    elif how is 'mmle':\n",
    "        prc = [x for x in percent]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        if major_mode=='percent':\n",
    "            Majors = len([x for x in prc if x>=major_thr])\n",
    "            Minors = len(prc) - Majors\n",
    "        elif major_mode=='number':\n",
    "            Majors = len(prc[0:min(major_thr,len(prc))])\n",
    "            Minors = len(prc) - Majors\n",
    "        payload = {'numberofplayers': Majors,#majo3\n",
    "                   'numberofplayers2': Minors,#minor\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}        \n",
    "        \n",
    "        \n",
    "    elif how is 'concentrated':\n",
    "        prc = [x for x in percent]\n",
    "        unassigned = 100 - sum(prc)\n",
    "        cons_point = concentration_point\n",
    "        number = int(np.floor(unassigned/cons_point))\n",
    "        residual = np.round(unassigned - number*cons_point,2)\n",
    "        prc = prc+[cons_point]*number+[residual]\n",
    "        prc_str = ''\n",
    "        for x in prc:\n",
    "            prc_str+=str(x)\n",
    "            prc_str+=' '\n",
    "        if major_mode=='percent':\n",
    "            Majors = len([x for x in prc if x>=major_thr])\n",
    "            Minors = len(prc) - Majors\n",
    "        elif major_mode=='number':\n",
    "            Majors = len(prc[0:min(major_thr,len(prc))])\n",
    "            Minors = len(prc) - Majors\n",
    "        payload = {'numberofplayers': Majors,#majo3\n",
    "                   'numberofplayers2': Minors,#minor\n",
    "                   'quota': quota,\n",
    "                   'textarea': prc_str}    \n",
    "    \n",
    "    \n",
    "    # website url\n",
    "    url = URL_banzhaf[how]\n",
    "    \n",
    "    # Making request\n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, data = payload)\n",
    "    except:\n",
    "        return('Error: request error!')\n",
    "    if time_pnt:\n",
    "        print('It took about ',np.round(response.elapsed.microseconds/1000000,2), 'seconds')\n",
    "        \n",
    "    # Parshing output html of wevsite\n",
    "    parsed_html = BeautifulSoup(response.text.encode('utf8'))\n",
    "    \n",
    "     # Finding rows or error message\n",
    "    if parsed_html('tr'):\n",
    "        rows = parsed_html('tr')\n",
    "    else:\n",
    "        return('Error: '+parsed_html.find('p').text)\n",
    "    \n",
    "    # Extracting rows to a list of lists\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        if row.th:\n",
    "            cols = row.find_all('th')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            if len(cols) is 1:\n",
    "                data.append([cols[0],''])\n",
    "            else:\n",
    "                data.append([ele for ele in cols if ele]) # Get rid of empty valuespty values\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            data.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "    \n",
    "    # Converting list of lists to a dataframe\n",
    "    try:\n",
    "        out = pd.DataFrame(data[1:], columns=data[0])\n",
    "        out.iloc[:,0] = prc        \n",
    "        out.rename(columns={'Absolute Banzhaf Index \\n\\n(Penrose Index)':'Abs_Banzhaf',\n",
    "                            'Normalised Banzhaf Index':'Norm_Banzhaf',\n",
    "                            'Coleman\\'s\\nPower to Prevent Action':'Coleman_Prevent',\n",
    "                            'Coleman\\'s Power to Initiate Action':'Coleman_Initiate'},inplace=True)\n",
    "        return(out)\n",
    "    \n",
    "    except:\n",
    "        return('Error: creating dataFrame error! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gameTheoric_concentration(symbol, index='shapley', how='concentrated',quota = 50.01,major_mode='number',major_thr=20,\n",
    "                              concentration_point=0.99,out_index = 'Largest',fast_mode = True):\n",
    "    \n",
    "    if how not in ['dispersion','concentrated']:\n",
    "        raise('how must be in [\\'dispersion\\',\\'concentrated\\']!')\n",
    "        \n",
    "    if index not in ['shapely','banzhaf']:\n",
    "        raise('how must be in [\\'shapley\\',\\'banzhaf\\']!')\n",
    "        \n",
    "    \n",
    "    percent_list = list(SDATA.percent[SDATA.Symbol==sym_list[34]])\n",
    "    \n",
    "    # Finding functions\n",
    "    func = {'shpley':find_shapley,'banzhaf':find_banzhaf}['index']\n",
    "    \n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-Shapley-Shubik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-Banzhaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_shapley(list(SDATA.percent[SDATA.Symbol==sym_list[12]]),how='ocean',quota = 50.01,major_mode='number',major_thr=20,time_pnt=True,fast_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# find_banzhaf(list(SDATA.percent[SDATA.Symbol==sym_list[34]]),how='concentrated',quota = 50.01,major_mode='number',major_thr=20,time_pnt=True,fast_mode=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
