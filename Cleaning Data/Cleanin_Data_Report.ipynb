{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Cleaning Data</center>\n",
    "### <center>by Mahdi Shahrabi</center>\n",
    "In this report I will describe how to clean shareholder data crawled from TSE, and merge it with price data and the data of volumne of trades done by Individula/istitutional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Modifying Shareholders Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this libraries to manipulate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import jdatetime as jd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Printing All Results\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have Farsi/Arabaci strings in our data, we use this function to convert all arabic characters to farsi characters. We run this function on all Farsi/Arabaci strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Arbic Characters to Persian Characters !\n",
    "from convert_ar_characters import convert_ar_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load data of daily shareholders from the folder it is saved in. In Nov 2019 the data is stored in about 110 chunks. However in each there are duplicated roww which we drop when we load each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations after dropping duplicates is:  7185010  and befor was about  7385391\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Loading Data of Firm Holder (Chunks)\n",
    "os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\")\n",
    "# os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\\Sample\")\n",
    "\n",
    "# Creating the list of files in the workspace and loading them\n",
    "all_filenames = [i for i in glob.glob('*.csv')]\n",
    "# # For 1399 Data\n",
    "# os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\\1399\")\n",
    "# all_filenames = [i for i in glob.glob('*1399.csv')]\n",
    "\n",
    "DataOrg = pd.concat([pd.read_csv(f,index_col=0) for f in all_filenames ])\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg.drop_duplicates()\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "print('The number of observations after dropping duplicates is: ',len(DataOrg),' and befor was about ',lng )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, also, rename columns to names that we use frequently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "DataOrg = DataOrg.rename(columns={'date':'Date_gre','group_name':'Industry','holder':'Shareholder_raw',\n",
    "                                  'Holder':'Shareholder_raw','Name':'Symbol','Percent':'percent','Number':'quantity',\n",
    "                                  'name':'Symbol','stock_id':'Id_tse','title':'firm_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert strings in DataOrg to farsi characters. Since, applying function in each cell is time-cosuming we use dictionary and mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DataOrg.firm_GPname: convert_ar_characters(x)\n",
    "Names = DataOrg.Industry.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Industry_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Industry'] = DataOrg.Industry.map(DataOrg_Industry_ArtoFa_dict)\n",
    "# # For 1399 Data\n",
    "# DataOrg['Industry'] = np.nan\n",
    "\n",
    "# DataOrg.Shareholder_raw: convert_ar_characters(x)\n",
    "Names = DataOrg.Shareholder_raw.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Shareholder_raw_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Shareholder_raw'] = DataOrg.Shareholder_raw.map(DataOrg_Shareholder_raw_ArtoFa_dict)\n",
    "\n",
    "# DataOrg.Symbol: convert_ar_characters(x)\n",
    "Names = DataOrg.Symbol.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Symbol_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Symbol'] = DataOrg.Symbol.map(DataOrg_Symbol_ArtoFa_dict)\n",
    "\n",
    "# DataOrg.firm_name: convert_ar_characters(x)\n",
    "Names = DataOrg.firm_name.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_firm_name_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['firm_name'] = DataOrg.firm_name.map(DataOrg_firm_name_ArtoFa_dict)\n",
    "# # For 1399 Data\n",
    "# DataOrg['firm_name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, also, creat Gregorian and Jalali date columns, along columns for year, month, and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Modifying Date\n",
    "DataOrg['True_Date'] = pd.to_datetime(DataOrg['Date_gre'], format='%Y%m%d')\n",
    "G = DataOrg.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "DataOrg_date_GtoJ_dict = dict(zip(G,J))\n",
    "DataOrg['Jalali_Date']=DataOrg.True_Date.map(DataOrg_date_GtoJ_dict)\n",
    "DataOrg['year'] = DataOrg.Jalali_Date.apply(lambda x: x.year)\n",
    "DataOrg['month'] = DataOrg.Jalali_Date.apply(lambda x: x.month)\n",
    "DataOrg['day'] = DataOrg.Jalali_Date.apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7185010"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "507782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # For with specific time\n",
    "# len(DataOrg)\n",
    "# temp1 = DataOrg[DataOrg.year<=1388]\n",
    "# DataOrg = temp1[temp1.year>=1386]\n",
    "# len(DataOrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load data of unique name of shareholder 'Sh_spec_handy.xlsx' and convert all Arabic characters to Farsi characters to find unique name of shareholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Setting current workspace to folder of Other Data\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\\_Data\\OtherData\")\n",
    "Sh_spec_handy=pd.read_excel(\"Sh_spec_handy.xlsx\")\n",
    "Sh_spec_handy.Shareholder_raw=Sh_spec_handy.Shareholder_raw.apply(lambda x: convert_ar_characters(x))\n",
    "Sh_spec_handy.Shareholder=Sh_spec_handy.Shareholder.apply(lambda x: convert_ar_characters(x))\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all shareholders name in our data has a name in 'Sh_spec_handy', we find new names and append it to 'Sh_spec_handy' and save it to \"NewSh_spec_handy.xlsx\". Then, we must complete this new list by hand, and use it for finding unique names in our shareholder data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of New Shareholder is:  0\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NewShH_raw=pd.DataFrame({\"Shareholder_raw\":list(set(DataOrg.Shareholder_raw.drop_duplicates())-set(Sh_spec_handy.Shareholder_raw))})\n",
    "# Adding to the List We Had Before and Saving it in an Excel File\n",
    "NewSh_spec_handy=Sh_spec_handy.append(NewShH_raw)\n",
    "NewSh_spec_handy.to_excel(\"NewSh_spec_handy.xlsx\")\n",
    "print('Number of New Shareholder is: ',len(NewShH_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this file, we creat a dictionary and find Shareholder unique names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 425 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating a Dictionary\n",
    "dict_ShH_raw_to_ShH = dict(zip(NewSh_spec_handy['Shareholder_raw'],NewSh_spec_handy['Shareholder']))\n",
    "dict_ShH_raw_to_Typ = dict(zip(NewSh_spec_handy['Shareholder_raw'],NewSh_spec_handy['Type']))\n",
    "# Maping\n",
    "DataOrg['ShareHolder'] = DataOrg['Shareholder_raw'].map(dict_ShH_raw_to_ShH)\n",
    "DataOrg['ShareHolder_Type'] = DataOrg['Shareholder_raw'].map(dict_ShH_raw_to_Typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging size is  507782 after merging size is  412293\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Meging shareholder with same sharehodler name but different sharehodler_raw name\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg.groupby(['Symbol','Jalali_Date','ShareHolder'],as_index=False).agg({'Id_tse':'first',\n",
    "                                                          'Industry':'first', 'percent':'sum',\n",
    "                                                          'quantity':'sum','Shareholder_raw':'count',  \n",
    "                                                          'ShareHolder_Type':'first','True_Date':'first', 'year':'first',\n",
    "                                                          'month':'first', 'day':'first'})\n",
    "print('Before merging size is ', lng, 'after merging size is ', len(DataOrg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find firm industry/Industry_Id, we use file \"name_groupName_groupId.xlsx\" which is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting arabic characters and renaming, we create a dictionary and map each of the industries to Industry_Id for DataOrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\\_Data\\OtherData\")\n",
    "firm_indus = pd.read_excel(\"name_groupName_groupId.xlsx\",index_col=0).drop(columns=['ID'])\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "firm_indus.head()\n",
    "\n",
    "## Maping Shareholder_raw to Symbol and Mapping \n",
    "firm_indus['Name'] = firm_indus.Name.apply(lambda x: convert_ar_characters(x))\n",
    "firm_indus['Gorup_name'] = firm_indus.Gorup_name.apply(lambda x: convert_ar_characters(x))\n",
    "firm_indus = firm_indus.rename(columns = {'Name':'Symbol','Gorup_name':'Industry','Group_id':'Industry_Id'})\n",
    "dict_Industry_Id = dict(zip(firm_indus.Industry.drop_duplicates(),firm_indus.Industry_Id.drop_duplicates()))\n",
    "# # For 1399 Data\n",
    "# DataOrg[\"Industry\"]=DataOrg.Symbol.map(dict(zip(firm_indus.Symbol,firm_indus.Industry)))\n",
    "DataOrg[\"Industry_Id\"]=DataOrg.Industry.map(dict_Industry_Id)\n",
    "DataOrg[\"Fill_Flag\"]=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that for some firms, in some days, the sum of percentage of its shareholders exceeds 100%! We find these firm-days and drop them. Later, we substitute them with last observed shareholders-percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations with percent sum over 100 is:  26\n",
      "number of observations before removing rows with percent-sum over 100 is:  412293   and after removing rows with percent-sum over 100 is:  412109\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Finding and Dropping rows of DataOrg which sum of percentage is larger that 100% !!\n",
    "temp = DataOrg.groupby(['Id_tse','Jalali_Date'],as_index=False).agg({'percent':'sum'})\n",
    "out_over100 = temp[temp.percent>100]\n",
    "\n",
    "# Printing output and sorting it\n",
    "out_over100.sort_values(by=['Id_tse','Jalali_Date']).head()\n",
    "print('number of observations with percent sum over 100 is: ',len(out_over100))\n",
    "\n",
    "# droping observations with sum over 100 (Time-Consuming, I should find a better way)\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg[~pd.Series(list(zip(DataOrg['Id_tse'], DataOrg['Jalali_Date']))).isin([(x,y) for (x,y) in zip(out_over100.Id_tse,out_over100.Jalali_Date)])]\n",
    "\n",
    "print('number of observations before removing rows with percent-sum over 100 is: ',lng,'  and after removing rows with percent-sum over 100 is: ',len(DataOrg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading other Data to Merge with shareholder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load data from file \"adjPrices_1399-01-09.csv\" which is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.59 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_gre</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>close</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150315</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>17995000</td>\n",
       "      <td>1102</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150316</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>524171</td>\n",
       "      <td>1157</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150317</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>572258</td>\n",
       "      <td>1215</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150318</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>200322</td>\n",
       "      <td>1276</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150325</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>185325</td>\n",
       "      <td>1339</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date_gre  High   Low  Open  Last    Volume  close                 ID\n",
       "0  20150315  1102  1102  1102  1102  17995000   1102  22941065011246116\n",
       "1  20150316  1157  1157  1157  1157    524171   1157  22941065011246116\n",
       "2  20150317  1215  1215  1215  1215    572258   1215  22941065011246116\n",
       "3  20150318  1276  1276  1276  1276    200322   1276  22941065011246116\n",
       "4  20150325  1339  1339  1339  1339    185325   1339  22941065011246116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\Price\\Adj price\")\n",
    "Adj_Price=pd.read_csv(\"adjPrices_1399-01-09.csv\",low_memory=False,index_col=0)\n",
    "\n",
    "os.chdir(r\"D:\\Shareholder Data\\Price\\Unadj price\")\n",
    "UnAdj_Price=pd.read_csv(\"Stocks_Prices_1399-02-06.csv\",low_memory=False,index_col=0)\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "Adj_Price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create Gregorian and Jalali date for adjusted and unadjusted price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Date\n",
    "Adj_Price['True_Date'] = pd.to_datetime(Adj_Price['Date_gre'], format='%Y%m%d')\n",
    "G = Adj_Price.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "Adj_Price_date_GtoJ_dict = dict(zip(G,J))\n",
    "Adj_Price['Jalali_Date']=Adj_Price.True_Date.map(Adj_Price_date_GtoJ_dict)\n",
    "## Preparing for Filtering by Date\n",
    "Adj_Price['year']=Adj_Price.Jalali_Date.apply(lambda x: x.year)\n",
    "Adj_Price['month']=Adj_Price.Jalali_Date.apply(lambda x: x.month)\n",
    "\n",
    "\n",
    "# Creating Date\n",
    "UnAdj_Price['True_Date'] = pd.to_datetime(UnAdj_Price['date'], format='%Y%m%d')\n",
    "G = UnAdj_Price.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "UnAdj_Price_date_GtoJ_dict = dict(zip(G,J))\n",
    "UnAdj_Price['Jalali_Date']=UnAdj_Price.True_Date.map(UnAdj_Price_date_GtoJ_dict)\n",
    "## Preparing for Filtering by Date\n",
    "UnAdj_Price['year']=UnAdj_Price.Jalali_Date.apply(lambda x: x.year)\n",
    "UnAdj_Price['month']=UnAdj_Price.Jalali_Date.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Merging Different kind of prices\n",
    "PRICE = pd.merge(Adj_Price,UnAdj_Price[['stock_id','Jalali_Date','close_price']],left_on=['ID','Jalali_Date'],right_on=['stock_id','Jalali_Date'],how='outer').rename(columns={'close_price':'Unadjusted_close'})\n",
    "PRICE.rename(columns={'ID':'Id_tse'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we load data of market index from file \"Index\", to hava a list of all woking dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\Shareholder Data\\Index\")\n",
    "Index=pd.read_csv(\"indexes_1399-01-12.csv\",low_memory=False,index_col=0)\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose usefull columns, create date, rename columns, reorder columns, and add a column \"Fill_Flag\" which, if ture, shows that if the data is filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing two needed columns\n",
    "Index = Index[Index.index_id=='overall_index'].reset_index(drop=True)\n",
    "\n",
    "# Creating Date Column\n",
    "\n",
    "Index['Jalali_Date']=Index.date.apply(lambda x: jd.date(year=int(x[0:x.find('/')]),\n",
    "                                                        month=int(x[(x.find('/')+1):x.rfind('/')]),\n",
    "                                                        day=int(x[(x.rfind('/')+1):])))\n",
    "\n",
    "Index['year'] = Index.Jalali_Date.apply(lambda x: x.year)\n",
    "Index['month'] = Index.Jalali_Date.apply(lambda x: x.month)\n",
    "Index['day'] = Index.Jalali_Date.apply(lambda x: x.day)\n",
    "\n",
    "Index['True_Date']=Index.Jalali_Date.apply(lambda x: jd.date.togregorian(x))\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Column\n",
    "Index = Index.rename(columns={'index':'close'})\n",
    "# Reordering Columns\n",
    "Index = Index[['Jalali_Date','close','True_Date','year','month']]\n",
    "# Flag for being filled\n",
    "Index['Fill_Flag']=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our work we must filter data by date using these lines of codes. However, we do not filter them yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations before filtering by date is:  412109\n",
      "The number of observations after filtering by date is:  412109\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Filtering Data by Date\n",
    "start_year = 1386\n",
    "start_month = 1\n",
    "end_year = 1398\n",
    "end_month = 12\n",
    "# Selecting Data From 1390, Farvardin and 1398 Esfand\n",
    "print('The number of observations before filtering by date is: ',len(DataOrg))\n",
    "DataOrg = DataOrg[np.logical_or(DataOrg.year>start_year,\n",
    "                                np.logical_and(DataOrg.year==start_year,DataOrg.month>=start_month))]\n",
    "DataOrg = DataOrg[np.logical_or(DataOrg.year<end_year,\n",
    "                                np.logical_and(DataOrg.year==end_year,DataOrg.month<=end_month))]\n",
    "print('The number of observations after filtering by date is: ',len(DataOrg))\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "Adj_Price = Adj_Price[np.logical_or(Adj_Price.year>start_year,\n",
    "                                    np.logical_and(Adj_Price.year==start_year,Adj_Price.month>=start_month))]\n",
    "Adj_Price = Adj_Price[np.logical_or(Adj_Price.year<end_year,\n",
    "                                    np.logical_and(Adj_Price.year==end_year,Adj_Price.month<=end_month))]\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "UnAdj_Price = UnAdj_Price[np.logical_or(UnAdj_Price.year>start_year,\n",
    "                                    np.logical_and(UnAdj_Price.year==start_year,UnAdj_Price.month>=start_month))]\n",
    "UnAdj_Price = UnAdj_Price[np.logical_or(UnAdj_Price.year<end_year,\n",
    "                                    np.logical_and(UnAdj_Price.year==end_year,UnAdj_Price.month<=end_month))]\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "Index = Index[np.logical_or(Index.year>start_year,\n",
    "                            np.logical_and(Index.year==start_year,Index.month>=start_month))]\n",
    "Index = Index[np.logical_or(Index.year<end_year,\n",
    "                            np.logical_and(Index.year==end_year,Index.month<=end_month))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Gaps in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ids in shareholder-daily data we fill all missed dates between dates that we have data for. We make its 'Fill_Flag' True. (Based on the 'stock' not the 'holder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id =  51106317433079213 :  0  from  248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id =  29747059672582491 :  100  from  248\n",
      "Id =  23086515493897579 :  200  from  248\n",
      "Number of filled rows in holder data is:  2015\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jalali_Date</th>\n",
       "      <th>Id_tse</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258220</th>\n",
       "      <td>1388-04-25</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303860</th>\n",
       "      <td>1388-02-22</td>\n",
       "      <td>778253364357513</td>\n",
       "      <td>وبملت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304161</th>\n",
       "      <td>1388-04-24</td>\n",
       "      <td>778253364357513</td>\n",
       "      <td>وبملت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297304</th>\n",
       "      <td>1388-04-25</td>\n",
       "      <td>1358190916156744</td>\n",
       "      <td>وآذر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229589</th>\n",
       "      <td>1388-02-19</td>\n",
       "      <td>2254054929817435</td>\n",
       "      <td>غدام</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Jalali_Date            Id_tse Symbol\n",
       "258220  1388-04-25   408934423224097  فرآور\n",
       "303860  1388-02-22   778253364357513  وبملت\n",
       "304161  1388-04-24   778253364357513  وبملت\n",
       "297304  1388-04-25  1358190916156744   وآذر\n",
       "229589  1388-02-19  2254054929817435   غدام"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Filling Holder Information\n",
    "# Initilizing the dataframe of filled data\n",
    "DataOrg_filter_stock_missed = pd.DataFrame(columns=DataOrg.columns.to_list())\n",
    "\n",
    "cnt = 0\n",
    "Ids_DataOrg = DataOrg.Id_tse.drop_duplicates()\n",
    "lng = len(Ids_DataOrg)\n",
    "\n",
    "for Id in Ids_DataOrg:\n",
    "    if cnt%100==0:\n",
    "        print('Id = ',Id,': ',cnt,' from ',lng)\n",
    "    # Selecting Data for stock x and setting fill_flag false again\n",
    "    data_sym = DataOrg[DataOrg.Id_tse==Id]\n",
    "    data_sym['Fill_Flag'] = False\n",
    "    # Initilizing Missed_Data for stock x\n",
    "    Missed_Data = pd.DataFrame(columns=data_sym.columns.to_list())\n",
    "    # Extracting dates availabe in holder information\n",
    "    Org_sym_Dates = pd.DataFrame(data_sym.Jalali_Date.drop_duplicates().sort_values(),columns=['Jalali_Date'])\n",
    "    # Extracting all working days\n",
    "    sym_dates_index=Index[np.logical_and(Index.Jalali_Date>=\n",
    "        Org_sym_Dates.Jalali_Date.iloc[0],Index.Jalali_Date<=Org_sym_Dates.Jalali_Date.iloc[-1])].reset_index(drop=True)\n",
    "    sym_dates_index = sym_dates_index.sort_values(by=['Jalali_Date'],ascending=True)\n",
    "    # Finding missed dates and sorting them\n",
    "    Missed_Dates=list(pd.DataFrame(set(sym_dates_index.Jalali_Date)-\n",
    "                    set(Org_sym_Dates.Jalali_Date),columns=['Jalali_Date']).sort_values(by='Jalali_Date').Jalali_Date)\n",
    "\n",
    "    # for all missed dates\n",
    "    for missdate in Missed_Dates:\n",
    "        # finding index of missdate in the list of woriking days\n",
    "        l = sym_dates_index[sym_dates_index.Jalali_Date==missdate].index.to_list()\n",
    "        # Initilizing the missed data for this day for stock x\n",
    "        Missed_Data_temp = pd.DataFrame(columns=data_sym.columns.to_list())\n",
    "        # choosing the data for fillingn missed data\n",
    "        data_for_filling = data_sym[data_sym.Jalali_Date == sym_dates_index.Jalali_Date.iloc[l[0]-1]]\n",
    "        # filling missed data\n",
    "        Missed_Data_temp[['Industry','Industry_Id', 'Shareholder_raw', 'Symbol', 'percent','quantity', 'Id_tse',\n",
    "                          'ShareHolder', 'ShareHolder_Type']] = data_for_filling[['Industry','Industry_Id', 'Shareholder_raw',\n",
    "                             'Symbol', 'percent','quantity', 'Id_tse', 'ShareHolder', 'ShareHolder_Type']]\n",
    "        # filling missed data - dates\n",
    "        Missed_Data_temp.Fill_Flag = True\n",
    "        Missed_Data_temp.Jalali_Date = missdate\n",
    "        Missed_Data_temp.True_Date = Missed_Data_temp.Jalali_Date.apply(lambda x: jd.date.togregorian(x))\n",
    "        Missed_Data_temp.Date_gre  = Missed_Data_temp.True_Date.apply(lambda x: x.year*10000+x.month*100+x.day)\n",
    "        Missed_Data_temp.year = Missed_Data_temp.Jalali_Date.apply(lambda x: x.year)\n",
    "        Missed_Data_temp.month  = Missed_Data_temp.Jalali_Date.apply(lambda x: x.month)\n",
    "        Missed_Data_temp.day  = Missed_Data_temp.Jalali_Date.apply(lambda x: x.day)\n",
    "        \n",
    "        # appending missed data for a day to missed data for stock x\n",
    "        Missed_Data = Missed_Data.append(Missed_Data_temp)\n",
    "    \n",
    "    # appending missed data for stock x to all missed data\n",
    "    DataOrg_filter_stock_missed = DataOrg_filter_stock_missed.append(Missed_Data)\n",
    "    cnt+=1\n",
    "\n",
    "# appending all missed data to data we have after filltering by stocks\n",
    "print('Number of filled rows in holder data is: ',len(DataOrg_filter_stock_missed))\n",
    "DATAORG = DataOrg.append(DataOrg_filter_stock_missed.drop_duplicates()).sort_values(by=['Symbol','Jalali_Date'])\n",
    "\n",
    "# List of Missing Days Symbols\n",
    "MissingDaySymbol = DATAORG[DATAORG.Fill_Flag==True].drop_duplicates(['Symbol','Jalali_Date']).sort_values(by=['Id_tse','Jalali_Date'])[['Jalali_Date','Id_tse','Symbol']]\n",
    "MissingDaySymbol.to_csv(\"MissingDaySymbol_List.csv\",index=False)\n",
    "MissingDaySymbol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 1399 Data\n",
    "# DATAORG = DataOrg.sort_values(by=['Symbol','Jalali_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\NumberOfShares\")\n",
    "# Creating the list of files in the workspace and loading them\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "NumberOfShares = pd.concat([pd.read_csv(f,index_col=0).drop_duplicates() for f in all_filenames ])\n",
    "\n",
    "# Creating Jalali Date\n",
    "NumberOfShares['True_Date'] = pd.to_datetime(NumberOfShares['date'], format='%Y%m%d')\n",
    "G = NumberOfShares.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "NumberOfShares_date_GtoJ_dict = dict(zip(G,J))\n",
    "NumberOfShares['Jalali_Date']=NumberOfShares.True_Date.map(NumberOfShares_date_GtoJ_dict)\n",
    "\n",
    "Names = NumberOfShares.name.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "NumberOfShares_name_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "NumberOfShares['name'] = NumberOfShares.name.map(NumberOfShares_name_ArtoFa_dict)\n",
    "\n",
    "NumberOfShares = NumberOfShares.rename(columns={'name':'Symbol'})\n",
    "\n",
    "NumberOfShares.head()\n",
    "\n",
    "# Merging\n",
    "DATAORG = pd.merge(DATAORG,NumberOfShares[['Shares','Symbol','Jalali_Date']],\n",
    "                  how='left',left_on=['Jalali_Date','Symbol'],right_on=['Jalali_Date','Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATAORG = pd.merge(DATAORG,PRICE[['Id_tse','Jalali_Date','High','Low','Open','Last','Volume','close','Unadjusted_close']],how='left',left_on=['Jalali_Date','Id_tse',],right_on=['Jalali_Date','Id_tse'])\n",
    "DATAORG['MarketCap'] = DATAORG['Shares']*DATAORG['Unadjusted_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Columns\n",
    "DATAORG = DATAORG[['Id_tse','Symbol', 'Jalali_Date','Industry', 'ShareHolder', 'percent',\n",
    "       'quantity', 'ShareHolder_Type',  'Shares','MarketCap', 'High', 'Low',\n",
    "       'Open', 'Last', 'Volume', 'close', 'Unadjusted_close', 'Industry_Id', 'True_Date', 'year',\n",
    "       'month', 'day','Fill_Flag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1399 Data \n",
    "Symbols = pd.DataFrame(DATAORG.Symbol.drop_duplicates(),columns=['Symbol'])\n",
    "Symbols['chnk_id'] = 13861388\n",
    "dict__ = dict(zip(Symbols.Symbol,Symbols.chnk_id))\n",
    "DATAORG['chnk_id'] = DATAORG.Symbol.map(dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Symbols = pd.DataFrame(DATAORG.Symbol.drop_duplicates(),columns=['Symbol'])\n",
    "Symbols['chnk_id'] =0\n",
    "cnt=0\n",
    "for i in range(0, len(Symbols), 5):\n",
    "    Symbols['chnk_id'][i:i+5] = cnt\n",
    "    cnt+=1\n",
    "dict__ = dict(zip(Symbols.Symbol,Symbols.chnk_id))\n",
    "DATAORG['chnk_id'] = DATAORG.Symbol.map(dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  from  1\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\Cleaned - Merged with Price\")\n",
    "gb = DATAORG.groupby(['chnk_id'])\n",
    "lng = len(gb)\n",
    "cnt=1\n",
    "for name, group in gb:\n",
    "    if cnt%5==1:\n",
    "        print(cnt,' from ',lng)\n",
    "    file_name = 'CHUNK_'+str(name)+'_DATA_Clean_Merged.csv'\n",
    "    group.to_csv(file_name)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
