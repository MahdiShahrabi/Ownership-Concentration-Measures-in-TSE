{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Cleaning Data</center>\n",
    "### <center>by Mahdi Shahrabi</center>\n",
    "In this report I will describe how to clean shareholder data crawled from TSE, and merge it with price data and the data of volumne of trades done by Individula/istitutional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Modifying Shareholders Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this libraries to manipulate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import jdatetime as jd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Printing All Results\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have Farsi/Arabaci strings in our data, we use this function to convert all arabic characters to farsi characters. We run this function on all Farsi/Arabaci strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing Arbic Characters to Persian Characters !\n",
    "from convert_ar_characters import convert_ar_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load data of daily shareholders from the folder it is saved in. In Nov 2019 the data is stored in about 110 chunks. However in each there are duplicated roww which we drop when we load each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations after dropping duplicates is:  7185010  and befor was about  7385391\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Loading Data of Firm Holder (Chunks)\n",
    "os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\")\n",
    "# os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\\Sample\")\n",
    "\n",
    "# Creating the list of files in the workspace and loading them\n",
    "all_filenames = [i for i in glob.glob('*.csv')]\n",
    "# # For 1399 Data\n",
    "# os.chdir(r\"D:\\Shareholder Data\\Sahreholders async html parser\\1399\")\n",
    "# all_filenames = [i for i in glob.glob('*1399.csv')]\n",
    "\n",
    "DataOrg = pd.concat([pd.read_csv(f,index_col=0) for f in all_filenames ])\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg.drop_duplicates()\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "print('The number of observations after dropping duplicates is: ',len(DataOrg),' and befor was about ',lng )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, also, rename columns to names that we use frequently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "DataOrg = DataOrg.rename(columns={'date':'Date_gre','group_name':'Industry','holder':'Shareholder_raw',\n",
    "                                  'Holder':'Shareholder_raw','Name':'Symbol','Percent':'percent','Number':'quantity',\n",
    "                                  'name':'Symbol','stock_id':'Id_tse','title':'firm_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert strings in DataOrg to farsi characters. Since, applying function in each cell is time-cosuming we use dictionary and mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DataOrg.firm_GPname: convert_ar_characters(x)\n",
    "Names = DataOrg.Industry.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Industry_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Industry'] = DataOrg.Industry.map(DataOrg_Industry_ArtoFa_dict)\n",
    "# # For 1399 Data\n",
    "# DataOrg['Industry'] = np.nan\n",
    "\n",
    "# DataOrg.Shareholder_raw: convert_ar_characters(x)\n",
    "Names = DataOrg.Shareholder_raw.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Shareholder_raw_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Shareholder_raw'] = DataOrg.Shareholder_raw.map(DataOrg_Shareholder_raw_ArtoFa_dict)\n",
    "\n",
    "# DataOrg.Symbol: convert_ar_characters(x)\n",
    "Names = DataOrg.Symbol.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_Symbol_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['Symbol'] = DataOrg.Symbol.map(DataOrg_Symbol_ArtoFa_dict)\n",
    "\n",
    "# DataOrg.firm_name: convert_ar_characters(x)\n",
    "Names = DataOrg.firm_name.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "DataOrg_firm_name_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "DataOrg['firm_name'] = DataOrg.firm_name.map(DataOrg_firm_name_ArtoFa_dict)\n",
    "# # For 1399 Data\n",
    "# DataOrg['firm_name'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, also, creat Gregorian and Jalali date columns, along columns for year, month, and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Modifying Date\n",
    "DataOrg['True_Date'] = pd.to_datetime(DataOrg['Date_gre'], format='%Y%m%d')\n",
    "G = DataOrg.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "DataOrg_date_GtoJ_dict = dict(zip(G,J))\n",
    "DataOrg['Jalali_Date']=DataOrg.True_Date.map(DataOrg_date_GtoJ_dict)\n",
    "DataOrg['year'] = DataOrg.Jalali_Date.apply(lambda x: x.year)\n",
    "DataOrg['month'] = DataOrg.Jalali_Date.apply(lambda x: x.month)\n",
    "DataOrg['day'] = DataOrg.Jalali_Date.apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load data of unique name of shareholder 'Sh_spec_handy.xlsx' and convert all Arabic characters to Farsi characters to find unique name of shareholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Setting current workspace to folder of Other Data\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\\_Data\\OtherData\")\n",
    "Sh_spec_handy=pd.read_excel(\"Sh_spec_handy.xlsx\")\n",
    "Sh_spec_handy.Shareholder_raw=Sh_spec_handy.Shareholder_raw.apply(lambda x: convert_ar_characters(x))\n",
    "Sh_spec_handy.Shareholder=Sh_spec_handy.Shareholder.apply(lambda x: convert_ar_characters(x))\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all shareholders name in our data has a name in 'Sh_spec_handy', we find new names and append it to 'Sh_spec_handy' and save it to \"NewSh_spec_handy.xlsx\". Then, we must complete this new list by hand, and use it for finding unique names in our shareholder data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2179\u001b[0m             \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2180\u001b[0m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2181\u001b[1;33m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2182\u001b[0m         )\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[0mneed_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m             \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m             \u001b[0mneed_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, engine, mode, **engine_kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Use the openpyxl module as the Excel writer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mfrom\u001b[0m \u001b[0mopenpyxl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWorkbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NewShH_raw=pd.DataFrame({\"Shareholder_raw\":list(set(DataOrg.Shareholder_raw.drop_duplicates())-set(Sh_spec_handy.Shareholder_raw))})\n",
    "# Adding to the List We Had Before and Saving it in an Excel File\n",
    "NewSh_spec_handy=Sh_spec_handy.append(NewShH_raw)\n",
    "NewSh_spec_handy.to_excel(\"NewSh_spec_handy.xlsx\")\n",
    "print('Number of New Shareholder is: ',len(NewShH_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this file, we creat a dictionary and find Shareholder unique names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating a Dictionary\n",
    "dict_ShH_raw_to_ShH = dict(zip(NewSh_spec_handy['Shareholder_raw'],NewSh_spec_handy['Shareholder']))\n",
    "dict_ShH_raw_to_Typ = dict(zip(NewSh_spec_handy['Shareholder_raw'],NewSh_spec_handy['Type']))\n",
    "# Maping\n",
    "DataOrg['ShareHolder'] = DataOrg['Shareholder_raw'].map(dict_ShH_raw_to_ShH)\n",
    "DataOrg['ShareHolder_Type'] = DataOrg['Shareholder_raw'].map(dict_ShH_raw_to_Typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging size is  7185010 after merging size is  5129548\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Meging shareholder with same sharehodler name but different sharehodler_raw name\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg.groupby(['Symbol','Jalali_Date','ShareHolder'],as_index=False).agg({'Id_tse':'first',\n",
    "                                                          'Industry':'first', 'percent':'sum',\n",
    "                                                          'quantity':'sum','Shareholder_raw':'count',  \n",
    "                                                          'ShareHolder_Type':'first','True_Date':'first', 'year':'first',\n",
    "                                                          'month':'first', 'day':'first'})\n",
    "print('Before merging size is ', lng, 'after merging size is ', len(DataOrg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find firm industry/Industry_Id, we use file \"name_groupName_groupId.xlsx\" which is as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting arabic characters and renaming, we create a dictionary and map each of the industries to Industry_Id for DataOrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\\_Data\\OtherData\")\n",
    "firm_indus = pd.read_excel(\"name_groupName_groupId.xlsx\",index_col=0).drop(columns=['ID'])\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "firm_indus.head()\n",
    "\n",
    "## Maping Shareholder_raw to Symbol and Mapping \n",
    "firm_indus['Name'] = firm_indus.Name.apply(lambda x: convert_ar_characters(x))\n",
    "firm_indus['Gorup_name'] = firm_indus.Gorup_name.apply(lambda x: convert_ar_characters(x))\n",
    "firm_indus = firm_indus.rename(columns = {'Name':'Symbol','Gorup_name':'Industry','Group_id':'Industry_Id'})\n",
    "dict_Industry_Id = dict(zip(firm_indus.Industry.drop_duplicates(),firm_indus.Industry_Id.drop_duplicates()))\n",
    "# # For 1399 Data\n",
    "# DataOrg[\"Industry\"]=DataOrg.Symbol.map(dict(zip(firm_indus.Symbol,firm_indus.Industry)))\n",
    "DataOrg[\"Industry_Id\"]=DataOrg.Industry.map(dict_Industry_Id)\n",
    "DataOrg[\"Fill_Flag\"]=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that for some firms, in some days, the sum of percentage of its shareholders exceeds 100%! We find these firm-days and drop them. Later, we substitute them with last observed shareholders-percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations with percent sum over 100 is:  176\n",
      "number of observations before removing rows with percent-sum over 100 is:  5129548   and after removing rows with percent-sum over 100 is:  5128083\n",
      "Wall time: 6min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Finding and Dropping rows of DataOrg which sum of percentage is larger that 100% !!\n",
    "temp = DataOrg.groupby(['Id_tse','Jalali_Date'],as_index=False).agg({'percent':'sum'})\n",
    "out_over100 = temp[temp.percent>100]\n",
    "\n",
    "# Printing output and sorting it\n",
    "out_over100.sort_values(by=['Id_tse','Jalali_Date']).head()\n",
    "print('number of observations with percent sum over 100 is: ',len(out_over100))\n",
    "\n",
    "# droping observations with sum over 100 (Time-Consuming, I should find a better way)\n",
    "lng = len(DataOrg)\n",
    "DataOrg = DataOrg[~pd.Series(list(zip(DataOrg['Id_tse'], DataOrg['Jalali_Date']))).isin([(x,y) for (x,y) in zip(out_over100.Id_tse,out_over100.Jalali_Date)])]\n",
    "\n",
    "print('number of observations before removing rows with percent-sum over 100 is: ',lng,'  and after removing rows with percent-sum over 100 is: ',len(DataOrg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading other Data to Merge with shareholder Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load data from file \"adjPrices_1399-01-09.csv\" which is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.52 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_gre</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>close</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150315</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>1102</td>\n",
       "      <td>17995000</td>\n",
       "      <td>1102</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150316</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>1157</td>\n",
       "      <td>524171</td>\n",
       "      <td>1157</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150317</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>1215</td>\n",
       "      <td>572258</td>\n",
       "      <td>1215</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150318</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>1276</td>\n",
       "      <td>200322</td>\n",
       "      <td>1276</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150325</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>1339</td>\n",
       "      <td>185325</td>\n",
       "      <td>1339</td>\n",
       "      <td>22941065011246116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date_gre  High   Low  Open  Last    Volume  close                 ID\n",
       "0  20150315  1102  1102  1102  1102  17995000   1102  22941065011246116\n",
       "1  20150316  1157  1157  1157  1157    524171   1157  22941065011246116\n",
       "2  20150317  1215  1215  1215  1215    572258   1215  22941065011246116\n",
       "3  20150318  1276  1276  1276  1276    200322   1276  22941065011246116\n",
       "4  20150325  1339  1339  1339  1339    185325   1339  22941065011246116"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\Price\\Adj price\")\n",
    "Adj_Price=pd.read_csv(\"adjPrices_1399-01-09.csv\",low_memory=False,index_col=0)\n",
    "\n",
    "os.chdir(r\"D:\\Shareholder Data\\Price\\Unadj price\")\n",
    "UnAdj_Price=pd.read_csv(\"Stocks_Prices_1399-02-06.csv\",low_memory=False,index_col=0)\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")\n",
    "Adj_Price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create Gregorian and Jalali date for adjusted and unadjusted price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Date\n",
    "Adj_Price['True_Date'] = pd.to_datetime(Adj_Price['Date_gre'], format='%Y%m%d')\n",
    "G = Adj_Price.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "Adj_Price_date_GtoJ_dict = dict(zip(G,J))\n",
    "Adj_Price['Jalali_Date']=Adj_Price.True_Date.map(Adj_Price_date_GtoJ_dict)\n",
    "## Preparing for Filtering by Date\n",
    "Adj_Price['year']=Adj_Price.Jalali_Date.apply(lambda x: x.year)\n",
    "Adj_Price['month']=Adj_Price.Jalali_Date.apply(lambda x: x.month)\n",
    "\n",
    "\n",
    "# Creating Date\n",
    "UnAdj_Price['True_Date'] = pd.to_datetime(UnAdj_Price['date'], format='%Y%m%d')\n",
    "G = UnAdj_Price.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "UnAdj_Price_date_GtoJ_dict = dict(zip(G,J))\n",
    "UnAdj_Price['Jalali_Date']=UnAdj_Price.True_Date.map(UnAdj_Price_date_GtoJ_dict)\n",
    "## Preparing for Filtering by Date\n",
    "UnAdj_Price['year']=UnAdj_Price.Jalali_Date.apply(lambda x: x.year)\n",
    "UnAdj_Price['month']=UnAdj_Price.Jalali_Date.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Merging Different kind of prices\n",
    "PRICE = pd.merge(Adj_Price,UnAdj_Price[['stock_id','Jalali_Date','close_price']],left_on=['ID','Jalali_Date'],right_on=['stock_id','Jalali_Date'],how='outer').rename(columns={'close_price':'Unadjusted_close'})\n",
    "PRICE.rename(columns={'ID':'Id_tse'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we load data of market index from file \"Index\", to hava a list of all woking dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\Shareholder Data\\Index\")\n",
    "Index=pd.read_csv(\"indexes_1399-01-12.csv\",low_memory=False,index_col=0)\n",
    "os.chdir(r\"C:\\Users\\Mahdi\\Dr. Heidari - TSE\\WorkInHands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose usefull columns, create date, rename columns, reorder columns, and add a column \"Fill_Flag\" which, if ture, shows that if the data is filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing two needed columns\n",
    "Index = Index[Index.index_id=='overall_index'].reset_index(drop=True)\n",
    "\n",
    "# Creating Date Column\n",
    "\n",
    "Index['Jalali_Date']=Index.date.apply(lambda x: jd.date(year=int(x[0:x.find('/')]),\n",
    "                                                        month=int(x[(x.find('/')+1):x.rfind('/')]),\n",
    "                                                        day=int(x[(x.rfind('/')+1):])))\n",
    "\n",
    "Index['year'] = Index.Jalali_Date.apply(lambda x: x.year)\n",
    "Index['month'] = Index.Jalali_Date.apply(lambda x: x.month)\n",
    "Index['day'] = Index.Jalali_Date.apply(lambda x: x.day)\n",
    "\n",
    "Index['True_Date']=Index.Jalali_Date.apply(lambda x: jd.date.togregorian(x))\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Column\n",
    "Index = Index.rename(columns={'index':'close'})\n",
    "# Reordering Columns\n",
    "Index = Index[['Jalali_Date','close','True_Date','year','month']]\n",
    "# Flag for being filled\n",
    "Index['Fill_Flag']=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our work we must filter data by date using these lines of codes. However, we do not filter them yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations before filtering by date is:  5128083\n",
      "The number of observations after filtering by date is:  4394783\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Filtering Data by Date\n",
    "start_year = 1390\n",
    "start_month = 1\n",
    "end_year = 1398\n",
    "end_month = 12\n",
    "# Selecting Data From 1390, Farvardin and 1398 Esfand\n",
    "print('The number of observations before filtering by date is: ',len(DataOrg))\n",
    "DataOrg = DataOrg[np.logical_or(DataOrg.year>start_year,\n",
    "                                np.logical_and(DataOrg.year==start_year,DataOrg.month>=start_month))]\n",
    "DataOrg = DataOrg[np.logical_or(DataOrg.year<end_year,\n",
    "                                np.logical_and(DataOrg.year==end_year,DataOrg.month<=end_month))]\n",
    "print('The number of observations after filtering by date is: ',len(DataOrg))\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "Adj_Price = Adj_Price[np.logical_or(Adj_Price.year>start_year,\n",
    "                                    np.logical_and(Adj_Price.year==start_year,Adj_Price.month>=start_month))]\n",
    "Adj_Price = Adj_Price[np.logical_or(Adj_Price.year<end_year,\n",
    "                                    np.logical_and(Adj_Price.year==end_year,Adj_Price.month<=end_month))]\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "UnAdj_Price = UnAdj_Price[np.logical_or(UnAdj_Price.year>start_year,\n",
    "                                    np.logical_and(UnAdj_Price.year==start_year,UnAdj_Price.month>=start_month))]\n",
    "UnAdj_Price = UnAdj_Price[np.logical_or(UnAdj_Price.year<end_year,\n",
    "                                    np.logical_and(UnAdj_Price.year==end_year,UnAdj_Price.month<=end_month))]\n",
    "\n",
    "# Selecting Data From 1390, Farvardin and 1398 Sharivar\n",
    "Index = Index[np.logical_or(Index.year>start_year,\n",
    "                            np.logical_and(Index.year==start_year,Index.month>=start_month))]\n",
    "Index = Index[np.logical_or(Index.year<end_year,\n",
    "                            np.logical_and(Index.year==end_year,Index.month<=end_month))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Gaps in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ids in shareholder-daily data we fill all missed dates between dates that we have data for. We make its 'Fill_Flag' True. (Based on the 'stock' not the 'holder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id =  34973883374080119 :  0  from  729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id =  29758477602878557 :  100  from  729\n",
      "Id =  66726992874614788 :  200  from  729\n",
      "Id =  27668158733246204 :  300  from  729\n",
      "Id =  50652985928800943 :  400  from  729\n",
      "Id =  19040514831923530 :  500  from  729\n",
      "Id =  59486059679335017 :  600  from  729\n",
      "Id =  50341528161302545 :  700  from  729\n",
      "Number of filled rows in holder data is:  101966\n",
      "Wall time: 1h 28min 31s\n",
      "Parser   : 433 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jalali_Date</th>\n",
       "      <th>Id_tse</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2921792</th>\n",
       "      <td>1392-11-07</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923306</th>\n",
       "      <td>1393-12-27</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925400</th>\n",
       "      <td>1395-11-06</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925454</th>\n",
       "      <td>1395-12-03</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925937</th>\n",
       "      <td>1396-08-07</td>\n",
       "      <td>408934423224097</td>\n",
       "      <td>فرآور</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Jalali_Date           Id_tse Symbol\n",
       "2921792  1392-11-07  408934423224097  فرآور\n",
       "2923306  1393-12-27  408934423224097  فرآور\n",
       "2925400  1395-11-06  408934423224097  فرآور\n",
       "2925454  1395-12-03  408934423224097  فرآور\n",
       "2925937  1396-08-07  408934423224097  فرآور"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Filling Holder Information\n",
    "# Initilizing the dataframe of filled data\n",
    "DataOrg_filter_stock_missed = pd.DataFrame(columns=DataOrg.columns.to_list())\n",
    "\n",
    "cnt = 0\n",
    "Ids_DataOrg = DataOrg.Id_tse.drop_duplicates()\n",
    "lng = len(Ids_DataOrg)\n",
    "\n",
    "for Id in Ids_DataOrg:\n",
    "    if cnt%100==0:\n",
    "        print('Id = ',Id,': ',cnt,' from ',lng)\n",
    "    # Selecting Data for stock x and setting fill_flag false again\n",
    "    data_sym = DataOrg[DataOrg.Id_tse==Id]\n",
    "    data_sym['Fill_Flag'] = False\n",
    "    # Initilizing Missed_Data for stock x\n",
    "    Missed_Data = pd.DataFrame(columns=data_sym.columns.to_list())\n",
    "    # Extracting dates availabe in holder information\n",
    "    Org_sym_Dates = pd.DataFrame(data_sym.Jalali_Date.drop_duplicates().sort_values(),columns=['Jalali_Date'])\n",
    "    # Extracting all working days\n",
    "    sym_dates_index=Index[np.logical_and(Index.Jalali_Date>=\n",
    "        Org_sym_Dates.Jalali_Date.iloc[0],Index.Jalali_Date<=Org_sym_Dates.Jalali_Date.iloc[-1])].reset_index(drop=True)\n",
    "    sym_dates_index = sym_dates_index.sort_values(by=['Jalali_Date'],ascending=True)\n",
    "    # Finding missed dates and sorting them\n",
    "    Missed_Dates=list(pd.DataFrame(set(sym_dates_index.Jalali_Date)-\n",
    "                    set(Org_sym_Dates.Jalali_Date),columns=['Jalali_Date']).sort_values(by='Jalali_Date').Jalali_Date)\n",
    "\n",
    "    # for all missed dates\n",
    "    for missdate in Missed_Dates:\n",
    "        # finding index of missdate in the list of woriking days\n",
    "        l = sym_dates_index[sym_dates_index.Jalali_Date==missdate].index.to_list()\n",
    "        # Initilizing the missed data for this day for stock x\n",
    "        Missed_Data_temp = pd.DataFrame(columns=data_sym.columns.to_list())\n",
    "        # choosing the data for fillingn missed data\n",
    "        data_for_filling = data_sym[data_sym.Jalali_Date == sym_dates_index.Jalali_Date.iloc[l[0]-1]]\n",
    "        # filling missed data\n",
    "        Missed_Data_temp[['Industry','Industry_Id', 'Shareholder_raw', 'Symbol', 'percent','quantity', 'Id_tse',\n",
    "                          'ShareHolder', 'ShareHolder_Type']] = data_for_filling[['Industry','Industry_Id', 'Shareholder_raw',\n",
    "                             'Symbol', 'percent','quantity', 'Id_tse', 'ShareHolder', 'ShareHolder_Type']]\n",
    "        # filling missed data - dates\n",
    "        Missed_Data_temp.Fill_Flag = True\n",
    "        Missed_Data_temp.Jalali_Date = missdate\n",
    "        Missed_Data_temp.True_Date = Missed_Data_temp.Jalali_Date.apply(lambda x: jd.date.togregorian(x))\n",
    "        Missed_Data_temp.Date_gre  = Missed_Data_temp.True_Date.apply(lambda x: x.year*10000+x.month*100+x.day)\n",
    "        Missed_Data_temp.year = Missed_Data_temp.Jalali_Date.apply(lambda x: x.year)\n",
    "        Missed_Data_temp.month  = Missed_Data_temp.Jalali_Date.apply(lambda x: x.month)\n",
    "        Missed_Data_temp.day  = Missed_Data_temp.Jalali_Date.apply(lambda x: x.day)\n",
    "        \n",
    "        # appending missed data for a day to missed data for stock x\n",
    "        Missed_Data = Missed_Data.append(Missed_Data_temp)\n",
    "    \n",
    "    # appending missed data for stock x to all missed data\n",
    "    DataOrg_filter_stock_missed = DataOrg_filter_stock_missed.append(Missed_Data)\n",
    "    cnt+=1\n",
    "\n",
    "# appending all missed data to data we have after filltering by stocks\n",
    "print('Number of filled rows in holder data is: ',len(DataOrg_filter_stock_missed))\n",
    "DATAORG = DataOrg.append(DataOrg_filter_stock_missed.drop_duplicates()).sort_values(by=['Symbol','Jalali_Date'])\n",
    "\n",
    "# List of Missing Days Symbols\n",
    "MissingDaySymbol = DATAORG[DATAORG.Fill_Flag==True].drop_duplicates(['Symbol','Jalali_Date']).sort_values(by=['Id_tse','Jalali_Date'])[['Jalali_Date','Id_tse','Symbol']]\n",
    "MissingDaySymbol.to_csv(\"MissingDaySymbol_List.csv\",index=False)\n",
    "MissingDaySymbol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 1399 Data\n",
    "# DATAORG = DataOrg.sort_values(by=['Symbol','Jalali_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\NumberOfShares\")\n",
    "# Creating the list of files in the workspace and loading them\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format('csv'))]\n",
    "NumberOfShares = pd.concat([pd.read_csv(f,index_col=0).drop_duplicates() for f in all_filenames ])\n",
    "\n",
    "# Creating Jalali Date\n",
    "NumberOfShares['True_Date'] = pd.to_datetime(NumberOfShares['date'], format='%Y%m%d')\n",
    "G = NumberOfShares.True_Date.drop_duplicates()\n",
    "J = G.apply(lambda x: jd.date.fromgregorian(day=x.day,month=x.month,year=x.year))\n",
    "NumberOfShares_date_GtoJ_dict = dict(zip(G,J))\n",
    "NumberOfShares['Jalali_Date']=NumberOfShares.True_Date.map(NumberOfShares_date_GtoJ_dict)\n",
    "\n",
    "Names = NumberOfShares.name.drop_duplicates()\n",
    "Conv_Names = Names.apply(lambda x : convert_ar_characters(x))\n",
    "NumberOfShares_name_ArtoFa_dict = dict(zip(Names,Conv_Names))\n",
    "NumberOfShares['name'] = NumberOfShares.name.map(NumberOfShares_name_ArtoFa_dict)\n",
    "\n",
    "NumberOfShares = NumberOfShares.rename(columns={'name':'Symbol'})\n",
    "\n",
    "NumberOfShares.head()\n",
    "\n",
    "# Merging\n",
    "DATAORG = pd.merge(DATAORG,NumberOfShares[['Shares','Symbol','Jalali_Date']],\n",
    "                  how='left',left_on=['Jalali_Date','Symbol'],right_on=['Jalali_Date','Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 53s\n",
      "Compiler : 406 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATAORG = pd.merge(DATAORG,PRICE[['Id_tse','Jalali_Date','High','Low','Open','Last','Volume','close','Unadjusted_close']],how='left',left_on=['Jalali_Date','Id_tse',],right_on=['Jalali_Date','Id_tse'])\n",
    "DATAORG['MarketCap'] = DATAORG['Shares']*DATAORG['Unadjusted_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Columns\n",
    "DATAORG = DATAORG[['Id_tse','Symbol', 'Jalali_Date','Industry', 'ShareHolder', 'percent',\n",
    "       'quantity', 'ShareHolder_Type',  'Shares','MarketCap', 'High', 'Low',\n",
    "       'Open', 'Last', 'Volume', 'close', 'Unadjusted_close', 'Industry_Id', 'True_Date', 'year',\n",
    "       'month', 'day','Fill_Flag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 1399 Data \n",
    "# Symbols = pd.DataFrame(DATAORG.Symbol.drop_duplicates(),columns=['Symbol'])\n",
    "# Symbols['chnk_id'] = 1399\n",
    "# dict__ = dict(zip(Symbols.Symbol,Symbols.chnk_id))\n",
    "# DATAORG['chnk_id'] = DATAORG.Symbol.map(dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahdi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Symbols = pd.DataFrame(DATAORG.Symbol.drop_duplicates(),columns=['Symbol'])\n",
    "Symbols['chnk_id'] =0\n",
    "cnt=0\n",
    "for i in range(0, len(Symbols), 5):\n",
    "    Symbols['chnk_id'][i:i+5] = cnt\n",
    "    cnt+=1\n",
    "dict__ = dict(zip(Symbols.Symbol,Symbols.chnk_id))\n",
    "DATAORG['chnk_id'] = DATAORG.Symbol.map(dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  from  144\n",
      "6  from  144\n",
      "11  from  144\n",
      "16  from  144\n",
      "21  from  144\n",
      "26  from  144\n",
      "31  from  144\n",
      "36  from  144\n",
      "41  from  144\n",
      "46  from  144\n",
      "51  from  144\n",
      "56  from  144\n",
      "61  from  144\n",
      "66  from  144\n",
      "71  from  144\n",
      "76  from  144\n",
      "81  from  144\n",
      "86  from  144\n",
      "91  from  144\n",
      "96  from  144\n",
      "101  from  144\n",
      "106  from  144\n",
      "111  from  144\n",
      "116  from  144\n",
      "121  from  144\n",
      "126  from  144\n",
      "131  from  144\n",
      "136  from  144\n",
      "141  from  144\n",
      "Wall time: 27min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir(r\"D:\\Shareholder Data\\Cleaned - Merged with Price\")\n",
    "gb = DATAORG.groupby(['chnk_id'])\n",
    "lng = len(gb)\n",
    "cnt=1\n",
    "for name, group in gb:\n",
    "    if cnt%5==1:\n",
    "        print(cnt,' from ',lng)\n",
    "    file_name = 'CHUNK_'+str(name)+'_DATA_Clean_Merged.csv'\n",
    "    group.to_csv(file_name)\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
